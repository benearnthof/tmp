https://arianhosseini.github.io/static/cv.pdf

human ears use the biomechanical analogue of a fourier transform to split sounds into their individual frequencies
we may use a model architecture to replicate this to embed sounds 
it is reasonable to assume that this brisk walk in the fourier domain also plays a part in the natural language understanding of the brain
thus the question arises, how we could make use of this for NLP
Hypothesis: It should be possible to enrich text data with speech (maybe even autogenerated speech) to allow us this walk in the fourier domain
It should also be possible to use "embedding trajectories" where we embed a sliding window of words similar to how one may obtain a spectrogramm from a sliding window over sound to embed raw text to trajectories and then look for similarities between trajectories for stuff like sentence embedding or context comparison in documents
we may be able to use adaptive context windows to obtain trajectories of standardized length

https://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf first mention of gradient clipping in the literature as "Truncating Gradients"
