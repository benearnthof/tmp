https://arianhosseini.github.io/static/cv.pdf

human ears use the biomechanical analogue of a fourier transform to split sounds into their individual frequencies
we may use a model architecture to replicate this to embed sounds 
it is reasonable to assume that this brisk walk in the fourier domain also plays a part in the natural language understanding of the brain
thus the question arises, how we could make use of this for NLP
Hypothesis: It should be possible to enrich text data with speech (maybe even autogenerated speech) to allow us this walk in the fourier domain
It should also be possible to use "embedding trajectories" where we embed a sliding window of words similar to how one may obtain a spectrogramm from a sliding window over sound to embed raw text to trajectories and then look for similarities between trajectories for stuff like sentence embedding or context comparison in documents
we may be able to use adaptive context windows to obtain trajectories of standardized length
Contrastive Learning of Musical Representations https://arxiv.org/pdf/2103.09410.pdf
Contrastive Learning of Audio Representations http://proceedings.mlr.press/v130/al-tahan21a/al-tahan21a.pdf

https://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf first mention of gradient clipping in the literature as "Truncating Gradients"

test for inductive priors of nns by passing through randomly init network once and evaluating on test data: 
  no training will take place, the initial scores should differ significantly from random scores if the architecture makes sense
  intuition: random projections of data should end up in similar regions of latent space if they have similar features 
    this may be expanded using "neural path kernels" to assign labels, so the entire path of the image through the layers would be of interest.
  problem with this: Architectures that are more flexible, read: have weaker inductive priors, will generalize better and would be weeded out by this approach because they may have worse initial scores but would perform better after training. 

On the equivalence between Neural Network and Support Vector Machine https://arxiv.org/pdf/2111.06063.pdf
  Also touches on Neural Tangent Kernels

http://www.catb.org/jargon/html/ 

Hopfield Networks & Transformers Blogpost 
https://ml-jku.github.io/hopfield-layers/

bayesian neural networks: 
http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html

full sky cmb estimation with nns
https://arxiv.org/pdf/2102.04327.pdf
https://www.astro.umd.edu/~miller/teaching/astr422/lecture21.pdf
https://plus.maths.org/content/cosmic-oracle-2

# contrastive embeddings based on mutual information maximization 
https://arxiv.org/pdf/2009.12061.pdf
# contrastive blog
https://lilianweng.github.io/lil-log/2021/05/31/contrastive-representation-learning.html
