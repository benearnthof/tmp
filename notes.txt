https://arianhosseini.github.io/static/cv.pdf

human ears use the biomechanical analogue of a fourier transform to split sounds into their individual frequencies
we may use a model architecture to replicate this to embed sounds 
it is reasonable to assume that this brisk walk in the fourier domain also plays a part in the natural language understanding of the brain
thus the question arises, how we could make use of this for NLP
Hypothesis: It should be possible to enrich text data with speech (maybe even autogenerated speech) to allow us this walk in the fourier domain
It should also be possible to use "embedding trajectories" where we embed a sliding window of words similar to how one may obtain a spectrogramm from a sliding window over sound to embed raw text to trajectories and then look for similarities between trajectories for stuff like sentence embedding or context comparison in documents
we may be able to use adaptive context windows to obtain trajectories of standardized length

https://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf first mention of gradient clipping in the literature as "Truncating Gradients"

test for inductive priors of nns by passing through randomly init network once and evaluating on test data: 
  no training will take place, the initial scores should differ significantly from random scores if the architecture makes sense
  intuition: random projections of data should end up in similar regions of latent space if they have similar features 
    this may be expanded using "neural path kernels" to assign labels, so the entire path of the image through the layers would be of interest.
  problem with this: Architectures that are more flexible, read: have weaker inductive priors, will generalize better and would be weeded out by this approach because they may have worse initial scores but would perform better after training. 
