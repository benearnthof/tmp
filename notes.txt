https://arianhosseini.github.io/static/cv.pdf

human ears use the biomechanical analogue of a fourier transform to split sounds into their individual frequencies
we may use a model architecture to replicate this to embed sounds 
it is reasonable to assume that this brisk walk in the fourier domain also plays a part in the natural language understanding of the brain
thus the question arises, how we could make use of this for NLP
Hypothesis: It should be possible to enrich text data with speech (maybe even autogenerated speech) to allow us this walk in the fourier domain
It should also be possible to use "embedding trajectories" where we embed a sliding window of words similar to how one may obtain a spectrogramm from a sliding window over sound to embed raw text to trajectories and then look for similarities between trajectories for stuff like sentence embedding or context comparison in documents
we may be able to use adaptive context windows to obtain trajectories of standardized length
Contrastive Learning of Musical Representations https://arxiv.org/pdf/2103.09410.pdf
Contrastive Learning of Audio Representations http://proceedings.mlr.press/v130/al-tahan21a/al-tahan21a.pdf

https://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf first mention of gradient clipping in the literature as "Truncating Gradients"

test for inductive priors of nns by passing through randomly init network once and evaluating on test data: 
  no training will take place, the initial scores should differ significantly from random scores if the architecture makes sense
  intuition: random projections of data should end up in similar regions of latent space if they have similar features 
    this may be expanded using "neural path kernels" to assign labels, so the entire path of the image through the layers would be of interest.
  problem with this: Architectures that are more flexible, read: have weaker inductive priors, will generalize better and would be weeded out by this approach because they may have worse initial scores but would perform better after training. 

On the equivalence between Neural Network and Support Vector Machine https://arxiv.org/pdf/2111.06063.pdf
  Also touches on Neural Tangent Kernels

http://www.catb.org/jargon/html/ 

Hopfield Networks & Transformers Blogpost 
https://ml-jku.github.io/hopfield-layers/

bayesian neural networks: 
http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html

full sky cmb estimation with nns
https://arxiv.org/pdf/2102.04327.pdf
https://www.astro.umd.edu/~miller/teaching/astr422/lecture21.pdf
https://plus.maths.org/content/cosmic-oracle-2

# contrastive embeddings based on mutual information maximization 
https://arxiv.org/pdf/2009.12061.pdf
# contrastive blog
https://lilianweng.github.io/lil-log/2021/05/31/contrastive-representation-learning.html
# neural ICDE for irregular time series 
https://arxiv.org/pdf/2005.08926.pdf
# on neural differential equations 
https://arxiv.org/pdf/2202.02435.pdf
# unsupervised time series embedding
https://proceedings.neurips.cc/paper/2019/file/53c6de78244e9f528eb3e1cda69699bb-Paper.pdf
# deep time series clustering: a review
https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjjuLa3wIH2AhWhwQIHHYTtDwkQFnoECB0QAQ&url=https%3A%2F%2Fwww.mdpi.com%2F2079-9292%2F10%2F23%2F3001%2Fpdf&usg=AOvVaw27nENBBhPlq3tpb2iKG5Sf

# A Span-Based Model for Joint Overlapped and Discontinuous Named Entity Recognition 
https://arxiv.org/pdf/2106.14373.pdf

# parametric umap for learning embeddings 
https://github.com/lmcinnes/umap/issues/831
# latent and generative models of animal vocalizations 
https://github.com/timsainb/avgn_paper

Transcending space and time:
fields.append({key: value['value'] if ent['fields'] else {} for ent in current_entities for key, value in zip(ent['fields'].keys(), ent['fields'].values())})
https://www.gwern.net/docs/www/arxiv.org/320270c4aa17a57178db0d1d0ebd3fe51883cd24.pdf
https://www.gwern.net/Clippy

https://sci-hub.se/10.1016/j.cognition.2015.03.016
# luss function for generative models
https://arxiv.org/pdf/2006.15057.pdf

http://cosmicpy.github.io/tutorials/gettingStarted.html

# detailed description of pixel based likelihood for CMB analysis
https://arxiv.org/pdf/1907.12875.pdf

# scattering networks to distinguish gaussian and nongaussian cmb data
https://arxiv.org/pdf/2102.02828.pdf
https://openreview.net/forum?id=bjy5Zb2fo2
  they plan to make the code accessible to the public
  implementation seems to be based on: 
    https://arxiv.org/pdf/2010.11661.pdf
    code is available on request 
 spin weighted spherical cnns as jax implementation
 https://github.com/google-research/google-research/tree/master/spin_spherical_cnns
    
